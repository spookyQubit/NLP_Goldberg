{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sentence Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-thought\n",
    "\n",
    "**Abstract of the paper**: We describe an approach for **unsupervised learning** of a generic, distributed sentence  encoder. Using  the  continuity of text from books, we **train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage**. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next **introduce a simple vocabulary expansion method to encode words that were not seen as part of training**, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.\n",
    "\n",
    "* This is similar to skip-gram, but instead of predicting words surrounding a traget word, one predicts senteneces surrounding target sentence. \n",
    "* The copora used is BookCorpus dataset.\n",
    "* During testing, one may encouter unseen words in the training corpus. In order to overcome this issue, a linear transformation is learnt between the training vocabulary repreentation and the word2vec representation. In case one encounters unseen words, one can look that word up in the word2vec and use the transformation before it can be fed into the skip-thought model.\n",
    "* Once representations for sentences are learnt, during evaluation, these representations are never non-linearly transformed.\n",
    "* The idea is to be able to preding the previous and the next sentence given the encoded representation of the middle sentence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Read\n",
    "  * [Multichannel Variable-Size Convolution for Sentence Classification](https://arxiv.org/pdf/1603.04513.pdf) by Yin et al.\n",
    "  * Siamese LSTM architechture: Best result so far for SICK data set for symantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
