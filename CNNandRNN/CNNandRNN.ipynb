{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW, the position of the words do not matter. The following sentences:\n",
    "\n",
    "    it was not good, it was actually quite bad\n",
    "    it was not bad, it was actually quite good\n",
    "\n",
    "are treated exactly the same under CBOW. This is why, ngram is a much more informative that bag-of-words.\n",
    "\n",
    "One can use an embedding on ngrams, leading to models of CBOW of bigrams for example. The issue with this is that the size of the embedding matrix will be exponential in n. So, if one does not have enough data, the bigrams: \"quite good\" and \"very good\" will not be similar enough even though they should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs are used to capture long distance relationships.\n",
    "\n",
    "Examples:\n",
    "   * Gender\n",
    "      * **He** does not have confidence on **himself**\n",
    "      * **She** does not have confidence on **herself**\n",
    "   * Reference to 'it' (Winograd Schema Challenge)\n",
    "      * The **trophy** will not fit in the suitcase because **it** was very big\n",
    "      * The trophy will not fit in the **suitcase** because **it** was very small\n",
    "      \n",
    "### What are RNNs used for:\n",
    "   * Read whole sentence and make a prediction, for example sentiment prediction. The difficulty here is that the model makes a prediction only at the end. So, RNN has to capture history well. \n",
    "   * Represent context within a sentence, for example pos tagging. This is not as hard to train as making a prediction at the end of the sentence.\n",
    "   \n",
    "### LSTM\n",
    "   * A solution to vanishing gradient issue. The basic idea is that the \n",
    "   \n",
    "### Efficiency and memory tricks\n",
    "   * Mini-batching: We generally batch sentences of nearly the same length together. We padd the small sentences to so that all sentences in the batch have same length. We then mask scores corresponding to the padded words. Then we sum to get the loss. One should shuffle inside each mini-batch update. \n",
    "   \n",
    "### Strengths/Weaknesses\n",
    "   * Quite flexible. Only recently CNN is showing more promise\n",
    "   * Require a **lot** of data\n",
    "   * Weak error signal passed from the end of sentence. This is specially a problem for sentence classification.\n",
    "  \n",
    "### Misc:\n",
    "   * Bi-RNN cannot be used to do language modeling because in bi-rnn, you would be conditioning on all the words in the sentence.\n",
    "   \n",
    "### Questions: \n",
    "   * In a language model, is it ok to use words from the left **and** words from the right contexts to predict the target word. According to Graham, one cannot do so and this is also the reason why bi-rnn cannot be used for language modeling. \n",
    "   * In a usual feed-forward NN, how does one choose the size of the layers? Suppose the input layer's dimension is 128, what should be picked up as the dimension for the next layer? Should it be more that 128 or less that 128? How much more/less? Same question for the second-last layer. Does the dimentsion of the last layer dictate the dimension of the second last layer? \n",
    "   * Read on vanishing gradient issue. How does LSTM solve this issue. According to Graham: LSTMS have additive connections (and not multiplicative connections) between time stamps which does not reduce the gradient.\n",
    "   * Truncated back propagation through time. Generally used for very long sequences. How would one implement this?\n",
    "   * Interpretation of RNN. Read the paper by Karapathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"./rnn_loss.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2841b32c50>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"./rnn_loss.pdf\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import dynet as dy\n",
    "import time\n",
    "from random import shuffle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of language-model using LSTM\n",
    "\n",
    "\n",
    "class LmRnn(object):\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.S = self.w2i[\"<s>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"]\n",
    "        self.train_data = list(self.read_train_data(train_path))\n",
    "        self.nWords = len(self.w2i)\n",
    "        self.test_data = list(self.read_test_data(test_path))\n",
    "        # Assert that reading the test data did not change the \n",
    "        # length of w2i\n",
    "        assert(self.nWords == len(self.w2i))\n",
    "        \n",
    "        # model parameters\n",
    "        self.WORD_EMB_SIZE = 64\n",
    "        self.HIDDEN_SIZE = 128\n",
    "        \n",
    "        # dynet model\n",
    "        self.model = dy.Model()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        \n",
    "        # dynet lookupp parameters for word embedding\n",
    "        self.words_lookup = self.model.add_lookup_parameters((self.nWords, \n",
    "                                                             self.WORD_EMB_SIZE))\n",
    "        \n",
    "        # dynet word level lstm\n",
    "        self.lstm = dy.LSTMBuilder(1, \n",
    "                                   self.WORD_EMB_SIZE, \n",
    "                                   self.HIDDEN_SIZE, \n",
    "                                   self.model)\n",
    "        \n",
    "        # dynet softmax weights\n",
    "        self.W_sm = self.model.add_parameters((self.nWords, \n",
    "                                               self.HIDDEN_SIZE))\n",
    "        self.b_sm = self.model.add_parameters(self.nWords)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        i = block_train_loss = block_train_words = all_time = 0\n",
    "        train_order = range(len(self.train_data))\n",
    "        for ITER in range(100):\n",
    "            shuffle(train_order)\n",
    "            for sid in train_order:\n",
    "                i += 1\n",
    "                if i % int(100) == 0:\n",
    "                    print(\"block_train_loss/block_train_words = {}\".format(math.exp(block_train_loss * 1./block_train_words)))\n",
    "                    print(\"elapsed time = {}\".format(time.time() - start_time))\n",
    "                    block_train_loss = 0\n",
    "                    block_train_words = 0\n",
    "                # get the loss for the current sentence\n",
    "                this_sent_loss_exp = self.calc_lm_loss(self.train_data[sid])\n",
    "                block_train_loss += this_sent_loss_exp.scalar_value() \n",
    "                block_train_words += len(self.train_data[sid])\n",
    "                this_sent_loss_exp.backward()\n",
    "                self.trainer.update()\n",
    "            print(\"epoch %r finished\" % ITER)\n",
    "            self.trainer.update_epoch(1.0)\n",
    "                \n",
    "                \n",
    "    def calc_lm_loss(self, sent):\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        # parameters -> exp\n",
    "        W_sm_exp = dy.parameter(self.W_sm)\n",
    "        b_sm_exp = dy.parameter(self.b_sm)\n",
    "        \n",
    "        # initialize the lstm\n",
    "        f_init = self.lstm.initial_state()\n",
    "        \n",
    "        # start the rnn by inputing \"<s>\"\n",
    "        s = f_init.add_input(self.words_lookup[self.S])\n",
    "        \n",
    "        losses = []\n",
    "        for wid in sent:\n",
    "            score = W_sm_exp * s.output() + b_sm_exp\n",
    "            loss = dy.pickneglogsoftmax(score, wid)\n",
    "            losses.append(loss)\n",
    "            s = s.add_input(self.words_lookup[wid])\n",
    "        return dy.esum(losses)\n",
    "    \n",
    "    def read_train_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                sent = [self.w2i[word] for word in line.strip().split(\" \")]\n",
    "                sent.append(self.S)\n",
    "                yield sent\n",
    "     \n",
    "    def read_test_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                sent = []\n",
    "                words = line.strip().split(\" \")\n",
    "                for word in words:\n",
    "                    if word in self.w2i:\n",
    "                        sent.append(self.w2i[word])\n",
    "                    else:\n",
    "                        sent.append(self.w2i[\"<unk>\"])\n",
    "                yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words = 18284\n",
      "block_train_loss/block_train_words = 4329.93120559\n",
      "elapsed time = 10.5359749794\n",
      "block_train_loss/block_train_words = 1126.19717092\n",
      "elapsed time = 21.7739989758\n",
      "block_train_loss/block_train_words = 682.788390579\n",
      "elapsed time = 32.3246450424\n",
      "block_train_loss/block_train_words = 638.526063686\n",
      "elapsed time = 42.8455340862\n",
      "block_train_loss/block_train_words = 648.263705471\n",
      "elapsed time = 53.7965459824\n",
      "block_train_loss/block_train_words = 522.830575695\n",
      "elapsed time = 64.267745018\n",
      "block_train_loss/block_train_words = 675.645164747\n",
      "elapsed time = 74.667525053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-b8609be7c932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of words = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlmRnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlmRnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-e89ba632eda0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mblock_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mthis_sent_loss_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mblock_train_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mthis_sent_loss_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch %r finished\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "lmRnn = LmRnn(\"../nn4nlp2017-code-master/data/classes/train.txt\", \n",
    "              \"../nn4nlp2017-code-master/data/classes/test.txt\")\n",
    "\n",
    "print(\"number of words = {}\".format(lmRnn.nWords))\n",
    "lmRnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "There are two things which we notice from the above output:\n",
    "   * The error is decreasing. So  our model is actually learning\n",
    "   * The time taken to process 100 sentences is around 10 sec\n",
    "   \n",
    "In the next part of the code, we will implement the same model but with mini-batching. The idea is to test whether mini-batching makes the processing faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"./rnn_minibatching.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2841b325d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"./rnn_minibatching.pdf\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "from collections import defaultdict\n",
    "import dynet as dy\n",
    "import time\n",
    "from random import shuffle\n",
    "import math\n",
    "\n",
    "# Implementation of language-model using LSTM\n",
    "\n",
    "\n",
    "class LmRnnMiniBatching(object):\n",
    "    def __init__(self, train_path, test_path):\n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.S = self.w2i[\"<s>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"]\n",
    "        self.train_data = list(self.read_train_data(train_path))\n",
    "        self.nWords = len(self.w2i)\n",
    "        self.test_data = list(self.read_test_data(test_path))\n",
    "        # Assert that reading the test data did not change the \n",
    "        # length of w2i\n",
    "        assert(self.nWords == len(self.w2i))\n",
    "        \n",
    "        # mini batch size\n",
    "        self.mini_batch_size = 32\n",
    "        \n",
    "        # model parameters\n",
    "        self.WORD_EMB_SIZE = 64\n",
    "        self.HIDDEN_SIZE = 128\n",
    "        \n",
    "        # dynet model\n",
    "        self.model = dy.Model()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        \n",
    "        # dynet lookupp parameters for word embedding\n",
    "        self.words_lookup = self.model.add_lookup_parameters((self.nWords, \n",
    "                                                             self.WORD_EMB_SIZE))\n",
    "        \n",
    "        # dynet word level lstm\n",
    "        self.lstm = dy.LSTMBuilder(1, \n",
    "                                   self.WORD_EMB_SIZE, \n",
    "                                   self.HIDDEN_SIZE, \n",
    "                                   self.model)\n",
    "        \n",
    "        # dynet softmax weights\n",
    "        self.W_sm = self.model.add_parameters((self.nWords, \n",
    "                                               self.HIDDEN_SIZE))\n",
    "        self.b_sm = self.model.add_parameters(self.nWords)\n",
    "        \n",
    "    def read_train_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                sent = [self.w2i[word] for word in line.strip().split(\" \")]\n",
    "                sent.append(self.S)\n",
    "                yield sent\n",
    "     \n",
    "    \n",
    "    def read_test_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                sent = []\n",
    "                words = line.strip().split(\" \")\n",
    "                for word in words:\n",
    "                    if word in self.w2i:\n",
    "                        sent.append(self.w2i[word])\n",
    "                    else:\n",
    "                        sent.append(self.w2i[\"<unk>\"])\n",
    "                yield sent\n",
    "    \n",
    "    \n",
    "    def get_first_sent_idx_of_batches(self):\n",
    "        \n",
    "        # For minibatching, we first need to sort the the train data\n",
    "        # This will minimize the number of words which will be masked in a minibatch\n",
    "        # See the above figure\n",
    "        self.train_data.sort(key=lambda x : -len(x))\n",
    "        \n",
    "        # Get the number of minibathes\n",
    "        num_minibatches = 0\n",
    "        if len(self.train_data)%self.mini_batch_size is 0:\n",
    "            num_minibatches = len(self.train_data)/self.mini_batch_size\n",
    "        else: \n",
    "            num_minibatches = len(self.train_data)/self.mini_batch_size + 1\n",
    "            \n",
    "        print(\"num_minibatches = {}\".format(num_minibatches))\n",
    "        \n",
    "        # bunch the minibatches together\n",
    "        # for example, train_order = a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        # batch size = 2\n",
    "        # first_sent_idx_of_batches = [0, 2, 4, 6, 8]\n",
    "        first_sent_idx_of_batches = [id * self.mini_batch_size for id in range(num_minibatches)] \n",
    "        \n",
    "        # shuffle the batches so that not all small sentences are trained at the beginning or at the end\n",
    "        shuffle(first_sent_idx_of_batches)\n",
    "        \n",
    "        # Now the snetences in a batch can be accessed via: \n",
    "        # for sid in first_sent_idx_of_batch:\n",
    "        #     print(train_order[sid:sid + self.mini_batch_size])\n",
    "        \n",
    "        return first_sent_idx_of_batches\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        first_sent_idx_of_batches = self.get_first_sent_idx_of_batches()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        i = block_train_loss = block_train_words = all_time = 0\n",
    "        train_order = range(len(self.train_data))\n",
    "        \n",
    "        for ITER in range(3):\n",
    "            shuffle(train_order)\n",
    "            for sid in first_sent_idx_of_batches:\n",
    "                i += 1\n",
    "                if i % int(1000 / self.mini_batch_size) == 0:\n",
    "                    print(\"block_train_loss/block_train_words = {}\".format(math.exp(block_train_loss * 1./block_train_words)))\n",
    "                    print(\"elapsed time = {}\".format(time.time() - start_time))\n",
    "                    print(\"sentences done = {}\".format((i*self.mini_batch_size)))\n",
    "                    block_train_loss = 0\n",
    "                    block_train_words = 0\n",
    "                # get the loss for the current sentence\n",
    "                this_batch_loss_exp, words_in_batch = self.calc_lm_loss(self.train_data[sid: sid + self.mini_batch_size])\n",
    "                block_train_loss += this_batch_loss_exp.scalar_value() \n",
    "                block_train_words += words_in_batch\n",
    "                this_batch_loss_exp.backward()\n",
    "                self.trainer.update()\n",
    "            print(\"epoch %r finished\" % ITER)\n",
    "            self.trainer.update_epoch(1.0)\n",
    "                \n",
    "                \n",
    "    def calc_lm_loss(self, sents):\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        # parameters -> exp\n",
    "        W_sm_exp = dy.parameter(self.W_sm)\n",
    "        b_sm_exp = dy.parameter(self.b_sm)\n",
    "        \n",
    "        # initialize the lstm\n",
    "        f_init = self.lstm.initial_state()\n",
    "        \n",
    "        # start the rnn by inputing \"<s>\"\n",
    "        s = f_init.add_input(self.words_lookup[self.S])\n",
    "        \n",
    "        # get the word ids and masks for each step\n",
    "        # Example: \n",
    "        # sents = [[1, 2, 3], [1, 2], [1]]\n",
    "        # wids = [[1, 1, 1], [2, 2, -1], [3, -1, -1]]\n",
    "        # masks = [[1, 1, 1], [1, 1, 0], [1, 0, 0]]\n",
    "        # where -1 is the assumed in this example as the index of S\n",
    "        tot_words = 0\n",
    "        wids = []\n",
    "        masks = []\n",
    "        for i in range(len(sents[0])):\n",
    "            # sents[0] because the 0th sent is the longest\n",
    "            wids.append([(sent[i] if len(sent)>i else self.S) for sent in sents])\n",
    "            mask = [(1 if len(sent)>i else 0) for sent in sents]\n",
    "            masks.append(mask)\n",
    "            tot_words += sum(mask)\n",
    "        \n",
    "        # Initial input tho the RNN has to be S\n",
    "        init_ids = [self.S] * len(sents)\n",
    "        s = f_init.add_input(dy.lookup_batch(self.words_lookup, init_ids))\n",
    "        \n",
    "        # Get losses by predicting the next word\n",
    "        losses = []\n",
    "        for wid, mask in zip(wids, masks):\n",
    "            # calculate the softmax loss\n",
    "            score = dy.affine_transform([b_sm_exp, W_sm_exp, s.output()])            \n",
    "            loss = dy.pickneglogsoftmax_batch(score, wid)\n",
    "            # mask the loss if any one sent is smaller\n",
    "            if mask[-1] != 1:\n",
    "                mask_expr = dy.inputVector(mask)\n",
    "                mask_expr = dy.reshape(mask_expr, (1, ), len(sents))\n",
    "                loss = loss * mask_expr\n",
    "            losses.append(loss)\n",
    "            s = s.add_input(dy.lookup_batch(self.words_lookup, wid))\n",
    "        return dy.sum_batches(dy.esum(losses)), tot_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words = 18284\n",
      "number of sentences = 8544\n",
      "num_minibatches = 267\n",
      "block_train_loss/block_train_words = 8756.20715213\n",
      "elapsed time = 7.77348709106\n",
      "sentences done = 992\n",
      "block_train_loss/block_train_words = 1514.63439409\n",
      "elapsed time = 14.4101791382\n",
      "sentences done = 1984\n",
      "block_train_loss/block_train_words = 1257.06479949\n",
      "elapsed time = 22.1578500271\n",
      "sentences done = 2976\n",
      "block_train_loss/block_train_words = 919.261884732\n",
      "elapsed time = 29.1969220638\n",
      "sentences done = 3968\n",
      "block_train_loss/block_train_words = 688.659342556\n",
      "elapsed time = 36.4949162006\n",
      "sentences done = 4960\n",
      "block_train_loss/block_train_words = 556.196629656\n",
      "elapsed time = 44.6118631363\n",
      "sentences done = 5952\n",
      "block_train_loss/block_train_words = 498.470694319\n",
      "elapsed time = 51.8876121044\n",
      "sentences done = 6944\n",
      "block_train_loss/block_train_words = 527.79080335\n",
      "elapsed time = 60.0206830502\n",
      "sentences done = 7936\n",
      "epoch 0 finished\n",
      "block_train_loss/block_train_words = 427.418137202\n",
      "elapsed time = 67.3311600685\n",
      "sentences done = 8928\n",
      "block_train_loss/block_train_words = 391.246989742\n",
      "elapsed time = 74.6578280926\n",
      "sentences done = 9920\n",
      "block_train_loss/block_train_words = 423.053942928\n",
      "elapsed time = 83.2348082066\n",
      "sentences done = 10912\n",
      "block_train_loss/block_train_words = 358.162370316\n",
      "elapsed time = 90.4120290279\n",
      "sentences done = 11904\n",
      "block_train_loss/block_train_words = 363.384958315\n",
      "elapsed time = 97.3938760757\n",
      "sentences done = 12896\n",
      "block_train_loss/block_train_words = 342.73462198\n",
      "elapsed time = 104.642749071\n",
      "sentences done = 13888\n",
      "block_train_loss/block_train_words = 348.926194315\n",
      "elapsed time = 112.121598005\n",
      "sentences done = 14880\n",
      "block_train_loss/block_train_words = 364.793132543\n",
      "elapsed time = 119.667901039\n",
      "sentences done = 15872\n",
      "block_train_loss/block_train_words = 335.49938162\n",
      "elapsed time = 126.424038172\n",
      "sentences done = 16864\n",
      "epoch 1 finished\n",
      "block_train_loss/block_train_words = 380.74088991\n",
      "elapsed time = 135.096032143\n",
      "sentences done = 17856\n",
      "block_train_loss/block_train_words = 294.685620967\n",
      "elapsed time = 141.810461044\n",
      "sentences done = 18848\n",
      "block_train_loss/block_train_words = 350.45048885\n",
      "elapsed time = 150.086730003\n",
      "sentences done = 19840\n",
      "block_train_loss/block_train_words = 305.660073505\n",
      "elapsed time = 157.455112219\n",
      "sentences done = 20832\n",
      "block_train_loss/block_train_words = 310.246114025\n",
      "elapsed time = 164.80568409\n",
      "sentences done = 21824\n",
      "block_train_loss/block_train_words = 284.559668576\n",
      "elapsed time = 171.812209129\n",
      "sentences done = 22816\n",
      "block_train_loss/block_train_words = 293.432375701\n",
      "elapsed time = 178.991909027\n",
      "sentences done = 23808\n",
      "block_train_loss/block_train_words = 310.345274582\n",
      "elapsed time = 186.638486147\n",
      "sentences done = 24800\n",
      "epoch 2 finished\n"
     ]
    }
   ],
   "source": [
    "lmRnnMb = LmRnnMiniBatching(\"../nn4nlp2017-code-master/data/classes/train.txt\", \n",
    "                            \"../nn4nlp2017-code-master/data/classes/test.txt\")\n",
    "\n",
    "print(\"number of words = {}\".format(lmRnnMb.nWords))\n",
    "print(\"number of sentences = {}\".format(len(lmRnnMb.train_data)))\n",
    "\n",
    "lmRnnMb.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch is fast!\n",
    "We clearly see that the minibatch is around 8~10 times faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}