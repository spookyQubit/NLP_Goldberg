{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "from collections import defaultdict\n",
    "import dynet as dy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_src_file = \"../nn4nlp2017-code-master/data/parallel/train.ja\"\n",
    "train_trg_file = \"../nn4nlp2017-code-master/data/parallel/train.en\"\n",
    "test_src_file = \"../nn4nlp2017-code-master/data/parallel/test.ja\"\n",
    "test_trg_file = \"../nn4nlp2017-code-master/data/parallel/test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
    "\n",
    "sos_sym = '<s>'\n",
    "eos_sym = '</s>'\n",
    "unk_sym = \"<unk>\"\n",
    "\n",
    "sos_src = w2i_src[sos_sym]\n",
    "sos_trg = w2i_trg[sos_sym]\n",
    "eos_src = w2i_src[eos_sym]\n",
    "eos_trg = w2i_trg[eos_sym]\n",
    "unk_src = w2i_src[unk_sym]\n",
    "unk_trg = w2i_trg[unk_sym]\n",
    "\n",
    "### Read the data\n",
    "def read_data(file_src, file_trg):\n",
    "    with open(file_src, \"r\") as f_src, open(file_trg, \"r\") as f_trg:\n",
    "        for l_src, l_trg in zip(f_src, f_trg):\n",
    "            s_src = [w2i_src[w] for w in l_src.strip().split() + [eos_sym]]\n",
    "            s_trg = [w2i_trg[w] for w in l_trg.strip().split() + [eos_sym]]\n",
    "            yield s_src, s_trg\n",
    "            \n",
    "train = list(read_data(train_src_file, train_trg_file))\n",
    "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
    "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
    "\n",
    "nWords_src = len(w2i_src) \n",
    "nWords_trg = len(w2i_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet\n",
    "model = dy.Model()\n",
    "trainier = dy.AdamTrainer(model)\n",
    "\n",
    "# Model parameters \n",
    "EMB_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "# Max sent len\n",
    "MAX_SENT_LEN = 50\n",
    "\n",
    "# Lookup parameters\n",
    "LOOK_UP_SRC = model.add_lookup_parameters((nWords_src, EMB_SIZE))\n",
    "LOOK_UP_TRG = model.add_lookup_parameters((nWords_trg, EMB_SIZE))\n",
    "\n",
    "# Word level GRUs\n",
    "GRU_BUILDER_SRC = dy.GRUBuilder(1, EMB_SIZE, HIDDEN_SIZE, model) \n",
    "GRU_BUILDER_TRG = dy.GRUBuilder(1, EMB_SIZE, HIDDEN_SIZE, model)\n",
    "\n",
    "# softmax from hidden state\n",
    "W_sm_p = model.add_parameters((nWords_trg, HIDDEN_SIZE))\n",
    "b_sm_p = model.add_parameters((nWords_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(sent):\n",
    "    sent_src = sent[0]\n",
    "    sent_trg = sent[1]\n",
    "    \n",
    "    # parameter ---> exp\n",
    "    W_sm_exp = dy.parameter(W_sm_p)\n",
    "    b_sm_exp = dy.parameter(b_sm_p)\n",
    "\n",
    "    # Encode the src sentence into an output vector\n",
    "    src_state = GRU_BUILDER_SRC.initial_state()\n",
    "    for w_id_src in sent_src:\n",
    "        src_state = src_state.add_input(LOOK_UP_SRC[w_id_src])\n",
    "    src_output = src_state.output()\n",
    "    \n",
    "    # Set the initial target gru state as the output of the source gru state\n",
    "    trg_state = GRU_BUILDER_TRG.initial_state().set_h([src_output])\n",
    "    \n",
    "    loss_exps = []\n",
    "    prev_word = sent_trg[0]\n",
    "    for w_id_trg in sent_trg[1:]:\n",
    "        trg_state = trg_state.add_input(LOOK_UP_TRG[prev_word])\n",
    "        loss_exps.append(dy.pickneglogsoftmax(dy.affine_transform([b_sm_exp, W_sm_exp, trg_state.output()]), \n",
    "                                              w_id_trg))\n",
    "        prev_word = w_id_trg\n",
    "    \n",
    "    return dy.esum(loss_exps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 10 sentences\n",
      "finished 20 sentences\n",
      "finished 30 sentences\n",
      "finished 40 sentences\n",
      "finished 50 sentences\n",
      "finished 60 sentences\n",
      "finished 70 sentences\n",
      "finished 80 sentences\n",
      "finished 90 sentences\n",
      "finished 100 sentences\n",
      "finished 110 sentences\n",
      "finished 120 sentences\n",
      "finished 130 sentences\n",
      "finished 140 sentences\n",
      "finished 150 sentences\n",
      "finished 160 sentences\n",
      "finished 170 sentences\n",
      "finished 180 sentences\n",
      "finished 190 sentences\n",
      "finished 200 sentences\n",
      "finished 210 sentences\n",
      "finished 220 sentences\n",
      "finished 230 sentences\n",
      "finished 240 sentences\n",
      "finished 250 sentences\n",
      "finished 260 sentences\n",
      "finished 270 sentences\n",
      "finished 280 sentences\n",
      "finished 290 sentences\n",
      "finished 300 sentences\n",
      "finished 310 sentences\n",
      "finished 320 sentences\n",
      "finished 330 sentences\n",
      "finished 340 sentences\n",
      "finished 350 sentences\n",
      "finished 360 sentences\n",
      "finished 370 sentences\n",
      "finished 380 sentences\n",
      "finished 390 sentences\n",
      "finished 400 sentences\n",
      "finished 410 sentences\n",
      "finished 420 sentences\n",
      "finished 430 sentences\n",
      "finished 440 sentences\n",
      "finished 450 sentences\n",
      "finished 460 sentences\n",
      "finished 470 sentences\n",
      "finished 480 sentences\n",
      "finished 490 sentences\n",
      "finished 500 sentences\n",
      "finished 510 sentences\n",
      "finished 520 sentences\n",
      "finished 530 sentences\n",
      "finished 540 sentences\n",
      "finished 550 sentences\n",
      "finished 560 sentences\n",
      "finished 570 sentences\n",
      "finished 580 sentences\n",
      "finished 590 sentences\n",
      "finished 600 sentences\n",
      "finished 610 sentences\n",
      "finished 620 sentences\n",
      "finished 630 sentences\n",
      "finished 640 sentences\n",
      "finished 650 sentences\n",
      "finished 660 sentences\n",
      "finished 670 sentences\n",
      "finished 680 sentences\n",
      "finished 690 sentences\n",
      "finished 700 sentences\n",
      "finished 710 sentences\n",
      "finished 720 sentences\n",
      "finished 730 sentences\n",
      "finished 740 sentences\n",
      "finished 750 sentences\n",
      "finished 760 sentences\n",
      "finished 770 sentences\n",
      "finished 780 sentences\n",
      "finished 790 sentences\n",
      "finished 800 sentences\n",
      "finished 810 sentences\n",
      "finished 820 sentences\n",
      "finished 830 sentences\n",
      "finished 840 sentences\n",
      "finished 850 sentences\n",
      "finished 860 sentences\n",
      "finished 870 sentences\n",
      "finished 880 sentences\n",
      "finished 890 sentences\n",
      "finished 900 sentences\n",
      "finished 910 sentences\n",
      "finished 920 sentences\n",
      "finished 930 sentences\n",
      "finished 940 sentences"
     ]
    }
   ],
   "source": [
    "for ITER in range(2):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0\n",
    "    train_words = 0\n",
    "    start = time.time()\n",
    "    for s_id, sent in enumerate(train):\n",
    "        loss_exp = calc_loss(sent)\n",
    "        train_loss += loss_exp.value()\n",
    "        train_words += len(sent)\n",
    "        loss_exp.backward()\n",
    "        trainier.update()\n",
    "        if (s_id+1) % 10 == 0:\n",
    "            print(\"finished {} sentences\".format(s_id + 1))\n",
    "    iter_time = time.time() - start\n",
    "    print(\"ITER = {}, train_loss/train_words = {}, time = {}\".format(ITER, \n",
    "                                                                     train_loss/train_words, \n",
    "                                                                     iter_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
