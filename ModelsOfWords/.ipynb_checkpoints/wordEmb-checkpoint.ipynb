{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "The purpose of word embeddings is to find a dense representation for each word in a vocabulary. In general, the way the distributed representation of a word is learnt is through a language model task in which each word is encoded as a vector. As an example, the language model task can be to predict a target word given the context around the word. \n",
    "\n",
    "Few advantages of a dense representation of words are:\n",
    "   * Once we have a dense vector representation, these can be used as an input to other models. These typically reduce the input dimention as compared to the case where words are encoded as one-hot-vectors.\n",
    "   * Using the dense representation, one can compute similartity socre between words. The idea is that \"words of a feather flock together\". So, if \"cat\" and \"dog\" occur in corpus in the same context, vector(\"cat\") and vector(\"dog\") will be similar. \n",
    "   * One can use the fact that similar words will have similar vecors to cluster words together. Clustering can be used to acceralate learning tasks as is done in \"A scalable hierarchical distributed language model\".\n",
    "\n",
    "A typical neural-network setup which is used to learn a word embedding is shown in the figure below. Here, the \"lookup\" (red vectors) and the rows of \"W\" matrix can be considered as word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"./wordEmbLanguageModel.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f198426e690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"./wordEmbLanguageModel.pdf\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of non-linearity\n",
    "\n",
    "Consider the sentences:\n",
    "   * Paris is a nice place to visit.\n",
    "   * I stayed at the Hilton.\n",
    "   * Paris Hilton appeared at the cover of Vogue. \n",
    "\n",
    "Note that \"Paris\" and \"Hilton\" in the first two sentences come in the context of travel. On the other hand \"Paris Hilton\" comes in the cotext of celebrity gossip. \n",
    "\n",
    "This is a prototypical example of an XOR situation. Such relationships can be learned only by introducing non-linearities in the model.\n",
    "\n",
    "For the sake of simplicity, we will not be adding non-linearity in the following models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "One can use a continuous bag of words language model to learn a distributed representation of words. The diagram dipicting the model is shown in the following figure. Note that\n",
    "   * The words in the context are added and not concatenated. This is ok if our primary task is not to learn the target word itself but just a vector representation of a word. \n",
    "   * Words to the right of the target word also come in the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"./wordEmbLanguageModelCBOW.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f198cebdfd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"./wordEmbLanguageModelCBOW.pdf\", width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Util function\n",
    "\n",
    "def get_context(sent, target_idx, n):\n",
    "    \"\"\"\n",
    "    Purpose: Given a sent (list) and a target_idx (int)\n",
    "             return the context. \n",
    "             sent = [\"Hello\", \"There\", \"I\", \"Am\", \"Here\"]\n",
    "             \n",
    "             target_idx = 0\n",
    "             return [0, 0, \"Hello\", \"There\"]\n",
    "             \n",
    "             target_idx = 1\n",
    "             return [0, \"Hello\", \"There\", \"I\"]\n",
    "             \n",
    "             target_idx = 4\n",
    "             return [\"I\", \"Am\", 0, 0]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the context\n",
    "    context = [S] * 2 * n\n",
    "    \n",
    "    write_idx = -1\n",
    "    \n",
    "    # populate context to the left of the target word\n",
    "    for idx in range(target_idx - n, target_idx):\n",
    "        write_idx += 1 \n",
    "        if idx >= 0:\n",
    "            context[write_idx] = sent[idx]\n",
    "            \n",
    "    # populate context to the right of the target word\n",
    "    for idx in range(target_idx + 1, target_idx + n + 1):\n",
    "        write_idx += 1\n",
    "        if idx < len(sent):\n",
    "            context[write_idx] = sent[idx]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordEmbCbow(object):\n",
    "    \n",
    "    def __init__(self, train_path):\n",
    "        \n",
    "        self.N = 2\n",
    "        self.EMB_SIZE = 32\n",
    "        \n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.S = self.w2i[\"<s>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"] \n",
    "        self.train = list(self.read_data(train_path))\n",
    "        self.w2i = defaultdict(lambda: self.UNK, self.w2i)\n",
    "        self.i2w = {i: w for w, i in self.w2i.iteritems()}\n",
    "        self.nWords = len(self.w2i)\n",
    "        \n",
    "        self.model = dy.Model()\n",
    "        self.trainer = dy.SimpleSGDTrainer(self.model)\n",
    "        \n",
    "        self.W_c_p = self.model.add_lookup_parameters((self.nWords, self.EMB_SIZE)) \n",
    "        self.W_w_p = self.model.add_parameters((self.nWords, self.EMB_SIZE))\n",
    "        \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                yield [self.w2i[word] for word in line.strip().split(\" \")]\n",
    "            \n",
    "    def calc_sent_loss(self, sent):\n",
    "        # Create a computation graph\n",
    "        dy.renew_cg()\n",
    "        W_w = dy.parameter(self.W_w_p)\n",
    "    \n",
    "        sent_loss = []\n",
    "        for i in range(len(sent)):\n",
    "            word_idx_in_context_window = get_context(sent, i, self.N)\n",
    "            context = dy.esum([dy.lookup(self.W_c_p, idx) for idx in word_idx_in_context_window])\n",
    "            scores = W_w * context\n",
    "            sent_loss.append(dy.pickneglogsoftmax(scores, i))\n",
    "        return dy.esum(sent_loss)\n",
    "    \n",
    "    def trainCbow(self, max_iter=100):\n",
    "        \n",
    "        for ITER in range(max_iter):\n",
    "            shuffle(self.train)\n",
    "            train_words = 0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            for sent_id, sent in enumerate(self.train):\n",
    "                # Each sent is a list of word indexes\n",
    "                # For each sentence, we calculate the \n",
    "                # loss in predicting the target words given the contexts. \n",
    "                # We perform a back propagation only once a sentence. \n",
    "                sent_loss = self.calc_sent_loss(sent)\n",
    "                train_loss += sent_loss.value()\n",
    "                train_words += len(sent) \n",
    "                sent_loss.backward()\n",
    "                self.trainer.update()\n",
    "                if (sent_id +1) % 1000 == 0:\n",
    "                    print(\"Finished processing {} sentences\".format(sent_id + 1))\n",
    "            print(\"ITER = {}, train_loss/words = {}, time = {}\".format(ITER, train_loss/train_words, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1000 sentences\n",
      "Finished processing 2000 sentences\n",
      "Finished processing 3000 sentences\n",
      "Finished processing 4000 sentences\n",
      "Finished processing 5000 sentences\n",
      "Finished processing 6000 sentences\n",
      "Finished processing 7000 sentences\n",
      "Finished processing 8000 sentences\n",
      "ITER = 0, train_loss/words = 2.93275829104, time = 89.0067739487\n"
     ]
    }
   ],
   "source": [
    "cbow = WordEmbCbow(\"../nn4nlp2017-code-master/data/classes/train.txt\")\n",
    "cbow.trainCbow(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram \n",
    "\n",
    "There are two ways of looking into Skip gram model:\n",
    "   * Given the target word, predict the words in the context. This is what is done in the Word2Vec.\n",
    "   * Treat each word in the context as independant and predict the word given the words in the context. This is what is explained in the book by Goldberg. \n",
    "   \n",
    "The schematic for skip-gram model for predicting the words in the context given a target word is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"./wordEmbLanguageModelSG.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f193e85f950>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"./wordEmbLanguageModelSG.pdf\", width=400, height=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordEmbSg(object):\n",
    "    def __init__(self, train_path):\n",
    "        \n",
    "        self.N = 2\n",
    "        self.EMB_SIZE = 32\n",
    "        \n",
    "        self.w2i = defaultdict(lambda: len(self.w2i))\n",
    "        self.S = self.w2i[\"<s>\"]\n",
    "        self.UNK = self.w2i[\"<unk>\"] \n",
    "        self.train = list(self.read_data(train_path))\n",
    "        self.w2i = defaultdict(lambda: self.UNK, self.w2i)\n",
    "        self.i2w = {i: w for w, i in self.w2i.iteritems()}\n",
    "        self.nWords = len(self.w2i)\n",
    "        \n",
    "        self.model = dy.Model()\n",
    "        self.trainer = dy.SimpleSGDTrainer(self.model)\n",
    "        \n",
    "        self.W_c_p = self.model.add_lookup_parameters((self.nWords, self.EMB_SIZE))\n",
    "        self.W_w_p = self.model.add_parameters((self.nWords, self.EMB_SIZE))\n",
    "        \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                yield [self.w2i[word] for word in line.strip().split(\" \")]\n",
    "    \n",
    "    def calc_sent_loss(self, sent):\n",
    "        # Create a computation graph\n",
    "        dy.renew_cg()\n",
    "        \n",
    "        W_w = dy.parameter(self.W_w_p)\n",
    "        all_losses = []\n",
    "        for i in range(len(sent)):\n",
    "            # Given the target word, calculate the \n",
    "            # log_probability of each word in the vocabulary\n",
    "            score = W_w * dy.lookup(self.W_c_p, sent[i])\n",
    "            log_prob = dy.log_softmax(score)\n",
    "            \n",
    "            # From the computed probability over entire vocabulary\n",
    "            # select the probability of the words in the context\n",
    "            word_idx_in_context_window = get_context(sent, i, self.N)\n",
    "            for context_idx in word_idx_in_context_window:\n",
    "                all_losses.append(-dy.pick(log_prob, context_idx))\n",
    "        return dy.esum(all_losses)\n",
    "    \n",
    "    def trainSg(self, max_iter=1):\n",
    "        \n",
    "        for ITER in range(max_iter):\n",
    "            shuffle(self.train)\n",
    "            train_words = 0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            for sent_id, sent in enumerate(self.train):\n",
    "                # Each sent is a list of word indexes\n",
    "                # For each sentence, we calculate the \n",
    "                # loss in predicting the the contexts given the target word. \n",
    "                # We perform a back propagation only once a sentence. \n",
    "                sent_loss = self.calc_sent_loss(sent)\n",
    "                train_loss += sent_loss.value()\n",
    "                train_words += len(sent) \n",
    "                sent_loss.backward()\n",
    "                self.trainer.update()\n",
    "                if (sent_id +1) % 1000 == 0:\n",
    "                    print(\"Finished processing {} sentences\".format(sent_id + 1))\n",
    "            print(\"ITER = {}, train_loss/words = {}, time = {}\".format(ITER, train_loss/train_words, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1000 sentences\n",
      "Finished processing 2000 sentences\n",
      "Finished processing 3000 sentences\n",
      "Finished processing 4000 sentences\n",
      "Finished processing 5000 sentences\n",
      "Finished processing 6000 sentences\n",
      "Finished processing 7000 sentences\n",
      "Finished processing 8000 sentences\n",
      "ITER = 0, train_loss/words = 29.2522201881, time = 105.120398045\n"
     ]
    }
   ],
   "source": [
    "sg = WordEmbSg(\"../nn4nlp2017-code-master/data/classes/train.txt\")\n",
    "sg.trainSg(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
