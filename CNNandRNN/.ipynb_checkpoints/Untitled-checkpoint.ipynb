{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CBOW, the position of the words do not matter and the following sentences:\n",
    "   * it was not good, it was actually quite bad\n",
    "   * it was not bad, it was actually quite good\n",
    "\n",
    "are treated exactly the same. This is why, ngram is a much more informative that bag-of-words.\n",
    "\n",
    "One can use an embedding on ngrams, leading to models of CBOW of bigrams for example. The issue with this is that the size of the embedding matrix will be exponential in n. So, if one does not have enough data, the bigrams: \"quite good\" and \"very good\" will not be similar enough even though they should be.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs are used to capture long distance relationships.\n",
    "\n",
    "Examples:\n",
    "   * Gender\n",
    "      * **He** does not have confidence on **himself**\n",
    "      * **Her** does not have confidence on **herself**\n",
    "   * Reference to 'it' (Winograd Schema Challenge)\n",
    "      * The trophy will not fit in the suitcase because **it** was very big\n",
    "      * The trophy will not fit in the suitcase because **it** was very small\n",
    "      \n",
    "### What are RNNs used for:\n",
    "   * Read whole sentence and make a prediction, for example sentiment prediction. The difficulty here is that the model makes a prediction only at the end. So, RNN has to capture history well. \n",
    "   * Represent context within a sentence, for example pos tagging. This is not as hard to train as making a prediction at the end of the sentence.\n",
    "  \n",
    "### Misc:\n",
    "   * Bi-RNN cannot be used to do language modeling because in bi-rnn, you would be conditioning on all the words in the sentence.\n",
    "   \n",
    "### Questions: \n",
    "   * In a language model, is it ok to use words from the left **and** words from the right contexts to predict the target word. According to Graham, one cannot do so and this is also the reason why bi-rnn cannot be used for language modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
